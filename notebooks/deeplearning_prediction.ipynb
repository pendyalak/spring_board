{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeplearning_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag_sCeykXck_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc509601-4147-4cac-c896-c68698f5ef80"
      },
      "source": [
        "!pip install -q --upgrade pip\r\n",
        "\r\n",
        "## installing the latest transformers version from pip\r\n",
        "!pip install --use-feature=2020-resolver -q transformers==3.0.2\r\n",
        "import transformers\r\n",
        "\r\n",
        "## installing Google Translator package\r\n",
        "!pip install -q googletrans"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.5MB 7.7MB/s \n",
            "\u001b[?25h\u001b[33mWARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 769 kB 9.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 883 kB 16.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 30.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 45.2 MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 981 kB 12.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 98 kB 7.4 MB/s \n",
            "\u001b[?25h  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDUmdu0fq2kI",
        "outputId": "4280065e-e493-4532-9c9e-6d5ee2306e98"
      },
      "source": [
        "import gc\r\n",
        "import os\r\n",
        "import random\r\n",
        "import transformers\r\n",
        "import warnings\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "\r\n",
        "from googletrans import Translator\r\n",
        "from pathlib import Path\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from tensorflow.keras import Model\r\n",
        "from tensorflow.keras.layers import Input, Dense\r\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from transformers import AutoTokenizer, TFAutoModel\r\n",
        "\r\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\r\n",
        "print(f\"Transformers version: {transformers.__version__}\")\r\n",
        "\r\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.3.0\n",
            "Transformers version: 3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_whJJs4QmRh"
      },
      "source": [
        "def encode_text(df, tokenizer, max_len, padding):\r\n",
        "    \"\"\"\r\n",
        "    Preprocessing textual data into encoded tokens.\r\n",
        "    \"\"\"\r\n",
        "    text = df[\"text\"].values.tolist()\r\n",
        "\r\n",
        "    # encoding text using tokenizer of the model\r\n",
        "    text_encoded = tokenizer.batch_encode_plus(\r\n",
        "        text,\r\n",
        "        pad_to_max_length = padding,\r\n",
        "        max_length = max_len\r\n",
        "    )\r\n",
        "\r\n",
        "    return text_encoded\r\n",
        "\r\n",
        "\r\n",
        "def get_tf_dataset(X, y, auto, labelled = True, repeat = False, shuffle = False, batch_size = 128):\r\n",
        "    \"\"\"\r\n",
        "    Creating tf.data.Dataset for TPU.\r\n",
        "    \"\"\"\r\n",
        "    if labelled:\r\n",
        "        ds = (tf.data.Dataset.from_tensor_slices((X[\"input_ids\"], y)))\r\n",
        "    else:\r\n",
        "        ds = (tf.data.Dataset.from_tensor_slices(X[\"input_ids\"]))\r\n",
        "\r\n",
        "    if repeat:\r\n",
        "        ds = ds.repeat()\r\n",
        "\r\n",
        "    if shuffle:\r\n",
        "        ds = ds.shuffle(2048)\r\n",
        "\r\n",
        "    ds = ds.batch(batch_size)\r\n",
        "    ds = ds.prefetch(auto)\r\n",
        "\r\n",
        "    return ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMZiZdi7-yQz"
      },
      "source": [
        "def build_model(model_name, max_len, learning_rate, metrics):\r\n",
        "    \"\"\"\r\n",
        "    Building the Deep Learning architecture\r\n",
        "    \"\"\"\r\n",
        "    # defining encoded inputs\r\n",
        "    input_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"input_ids\")\r\n",
        "    \r\n",
        "    # defining transformer model embeddings\r\n",
        "    transformer_model = TFAutoModel.from_pretrained(model_name)\r\n",
        "    transformer_embeddings = transformer_model(input_ids)[0]\r\n",
        "\r\n",
        "    # defining output layer\r\n",
        "    output_values = Dense(3, activation = \"softmax\")(transformer_embeddings[:, 0, :])\r\n",
        "\r\n",
        "    # defining model\r\n",
        "    model = Model(inputs = input_ids, outputs = output_values)\r\n",
        "    opt = Adam(learning_rate = learning_rate)\r\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\r\n",
        "    metrics = metrics\r\n",
        "\r\n",
        "    model.compile(optimizer = opt, loss = loss, metrics = metrics)\r\n",
        "\r\n",
        "    return model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x13nbxJF-0pr"
      },
      "source": [
        "def predict_test(model_name,test_data):\r\n",
        "    \"\"\"\r\n",
        "    Testing the model\r\n",
        "    \"\"\"\r\n",
        "    ## reading data\r\n",
        "    test_dict = {\"text\":[test_data]}\r\n",
        "    df_test = pd.DataFrame(test_dict,columns=[\"text\"])\r\n",
        "\r\n",
        "    X_test_encoded = encode_text(df = df_test, tokenizer = AutoTokenizer.from_pretrained(model_name), max_len = 128, padding = True)\r\n",
        "\r\n",
        "\r\n",
        "    # Build the model\r\n",
        "    model = build_model(model_name, 128, \"1e-5\", [\"sparse_categorical_accuracy\"])\r\n",
        "    model.load_weights(\"/content/drive/MyDrive/model.h5\")\r\n",
        "    \r\n",
        "    ds_test = get_tf_dataset(X_test_encoded,auto=tf.data.experimental.AUTOTUNE,labelled = False,y=-1,batch_size=64) #, -1, config.AUTO, labelled = False, batch_size = config.BATCH_SIZE * config.REPLICAS * 4)\r\n",
        "    print(np.argmax(model.predict(ds_test)))\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvgfrJIR-7Aw"
      },
      "source": [
        "import os\r\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \r\n",
        "import tensorflow as tf\r\n",
        "import logging\r\n",
        "logging.basicConfig(level=logging.ERROR)\r\n",
        "predict_test(\"bert-large-cased\",\"Still, as I urged our leaving Ireland with such inquietude and impatience, my father thought it best to yield.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}